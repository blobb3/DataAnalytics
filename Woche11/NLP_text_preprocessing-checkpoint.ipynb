{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Workspacezhaw\\data analytics\\Woche 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required\n",
    "#!pip install nltk==3.7\n",
    "#!pip install pprint\n",
    "\n",
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())\n",
    "\n",
    "# Version abfragen von nltk\n",
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac6140",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02bc6730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The base is overtaken and some warriors died in the progress. A brave warrior was send on the main quest to win against them. the base consists of a village located by the river.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your own documents Aufgabe 1b\n",
    "d1 = 'The base is overtaken and some warriors died in the progress.'\n",
    "d2 = 'A brave warrior was send on the main quest to win against them.'\n",
    "d3 = 'the base consists of a village located by the river.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb02a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the base is overtaken and some warriors died in the progress. a brave warrior was send on the main quest to win against them. the base consists of a village located by the river.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function (Grund: man möchte möglichst alles einheitlich haben im Text)\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9092b871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the base is overtaken and some warriors died in the progress a brave warrior was send on the main quest to win against them the base consists of a village located by the river'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "445e11b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'very', 'shan', 'an', 'what', 'myself', 'itself', 'my', 'herself', 'had', \"couldn't\", 'yourselves', \"you'll\", \"weren't\", \"you'd\", 'here', 'was', 'again', 'to', 'mustn', 'the', \"should've\", 'then', 'same', 'down', 'before', 'they', 'than', 'we', 'have', 'those', 'am', 'out', 'ain', 'with', 'above', 'hasn', 'ma', 'a', 'his', \"doesn't\", \"mustn't\", \"isn't\", \"won't\", 'yourself', 'why', 'theirs', 'now', 'other', 're', 'while', 'ourselves', 'on', 'just', 'o', 'our', 'at', 'up', 'as', 'most', 'will', \"shan't\", 'your', 'them', \"hasn't\", 'nor', 'so', 'such', 'her', 'aren', 't', 'should', 'only', \"wouldn't\", 'after', 'and', 'if', \"she's\", 'doing', 'themselves', 'their', 'been', 'weren', 'because', 'all', 'but', 'y', 'isn', 'both', 'be', 'himself', 'this', 'below', \"hadn't\", 'do', \"you're\", 'you', 'through', 'that', 'during', 'hadn', \"needn't\", 'shouldn', 'does', 'once', 'who', \"wasn't\", \"it's\", 'whom', 'from', 'no', 'these', \"aren't\", 'it', 'its', 'not', 'haven', 'yours', 'ours', 'don', 'where', 'll', 'having', 'were', \"you've\", 'some', 'he', 'any', 'over', 'between', 'd', 'didn', 'hers', \"that'll\", 'of', 'him', 'won', \"mightn't\", \"haven't\", \"shouldn't\", 'needn', 'can', 've', 'are', 'doesn', 'me', 'she', 'i', 'has', 'off', 'few', 's', 'couldn', 'in', 'for', 'into', 'is', 'wasn', \"didn't\", 'each', 'mightn', 'how', 'by', 'further', 'against', 'wouldn', 'own', 'more', 'when', 'did', 'until', 'or', \"don't\", 'which', 'too', 'm', 'there', 'being', 'under', 'about'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d14c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base', 'overtaken', 'warriors', 'died', 'progress', 'brave', 'warrior', 'send', 'main', 'quest', 'win', 'base', 'consists', 'village', 'located', 'river']"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d09f8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['base', 'overtaken', 'warriors', 'died', 'progress', 'brave', 'warrior', 'send', 'main', 'quest', 'win', 'base', 'consists', 'village', 'located', 'river'] \n",
      "\n",
      "After lemmatization:\n",
      "['base', 'overtake', 'warriors', 'die', 'progress', 'brave', 'warrior', 'send', 'main', 'quest', 'win', 'base', 'consist', 'village', 'locate', 'river']"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c5347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "# durch die Lemmatisierung wurde er Satz auf folgende Hauptwörter eingeschränkt => die Anführungsstriche sollten nur am Anfang/Ende sein (damit es erkennt, dass es ein Satz ist)\n",
    "corpus = ['base overtake warriors die progress', \n",
    "          'brave warrior send main quest win', \n",
    "          'base consist village locate river']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd096813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   base  brave  consist  die  locate  main  overtake  progress  quest  river  \\\n",
      "0     1      0        0    1       0     0         1         1      0      0   \n",
      "1     0      1        0    0       0     1         0         0      1      0   \n",
      "2     1      0        1    0       1     0         0         0      0      1   \n",
      "\n",
      "   send  village  warrior  warriors  win  \n",
      "0     0        0        0         1    0  \n",
      "1     1        0        1         0    1  \n",
      "2     0        1        0         0    0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1) - Unigramm = N-Gramm\n",
    "vectorizer = CountVectorizer(min_df=0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)\n",
    "\n",
    "#bycicle kommt zweimal vor | drive kommt in allen 3 Dkomenten vor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b215384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   base consist  base overtake  brave warrior  consist village  die progress  \\\n",
      "0             0              1              0                0             1   \n",
      "1             0              0              1                0             0   \n",
      "2             1              0              0                1             0   \n",
      "\n",
      "   locate river  main quest  overtake warriors  quest win  send main  \\\n",
      "0             0           0                  1          0          0   \n",
      "1             0           1                  0          1          1   \n",
      "2             1           0                  0          0          0   \n",
      "\n",
      "   village locate  warrior send  warriors die  \n",
      "0               0             0             1  \n",
      "1               0             1             0  \n",
      "2               1             0             0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2) mit 2- hat man zwei folgewörter (B-Gramm)\n",
    "vectorizer = CountVectorizer(min_df=0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c67b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 15 \n",
      "\n",
      "The words in the corpus: \n",
      " {'village', 'locate', 'river', 'main', 'progress', 'brave', 'warrior', 'die', 'consist', 'win', 'overtake', 'base', 'quest', 'send', 'warriors'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   village  locate  river    main  progress   brave  warrior  die  consist  \\\n",
      "0      0.0     0.0    0.0  0.0000       0.2  0.0000   0.0000  0.2      0.0   \n",
      "1      0.0     0.0    0.0  0.1667       0.0  0.1667   0.1667  0.0      0.0   \n",
      "2      0.2     0.2    0.2  0.0000       0.0  0.0000   0.0000  0.0      0.2   \n",
      "\n",
      "      win  overtake  base   quest    send  warriors  \n",
      "0  0.0000       0.2   0.2  0.0000  0.0000       0.2  \n",
      "1  0.1667       0.0   0.0  0.1667  0.1667       0.0  \n",
      "2  0.0000       0.0   0.2  0.0000  0.0000       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))\n",
    " \n",
    "# TF=> wie oft wöter vorkommen (wörter die häufig vokommen, werden heruntergewichtet, die die weniger vorkommen werden höher gewichtet)\n",
    "# am höchsten gewichtet wird das wort \"bycicle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c21a6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "        village:     0.4771\n",
      "         locate:     0.4771\n",
      "          river:     0.4771\n",
      "           main:     0.4771\n",
      "       progress:     0.4771\n",
      "          brave:     0.4771\n",
      "        warrior:     0.4771\n",
      "            die:     0.4771\n",
      "        consist:     0.4771\n",
      "            win:     0.4771\n",
      "       overtake:     0.4771\n",
      "           base:     0.1761\n",
      "          quest:     0.4771\n",
      "           send:     0.4771\n",
      "       warriors:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b0b8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   village  locate   river    main  progress   brave  warrior     die  \\\n",
      "0   0.0000  0.0000  0.0000  0.0000    0.0954  0.0000   0.0000  0.0954   \n",
      "1   0.0000  0.0000  0.0000  0.0795    0.0000  0.0795   0.0795  0.0000   \n",
      "2   0.0954  0.0954  0.0954  0.0000    0.0000  0.0000   0.0000  0.0000   \n",
      "\n",
      "   consist     win  overtake    base   quest    send  warriors  \n",
      "0   0.0000  0.0000    0.0954  0.0352  0.0000  0.0000    0.0954  \n",
      "1   0.0000  0.0795    0.0000  0.0000  0.0795  0.0795    0.0000  \n",
      "2   0.0954  0.0000    0.0000  0.0352  0.0000  0.0000    0.0000  \n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0875f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT', 'B-NP'),\n",
      " ('storm', 'NN', 'I-NP'),\n",
      " ('had', 'VBD', 'O'),\n",
      " ('felt', 'VBN', 'O'),\n",
      " ('like', 'IN', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('rumor', 'NN', 'I-NP'),\n",
      " ('all', 'DT', 'B-NP'),\n",
      " ('day', 'NN', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('but', 'CC', 'O'),\n",
      " ('now', 'RB', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('sky', 'NN', 'I-NP'),\n",
      " ('was', 'VBD', 'O'),\n",
      " ('delivering', 'VBG', 'O'),\n",
      " ('.', '.', 'O'),\n",
      " ('For', 'IN', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('second', 'JJ', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('like', 'IN', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('knife', 'NN', 'I-NP'),\n",
      " ('catching', 'VBG', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('glint', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('light', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('refracting', 'VBG', 'O'),\n",
      " ('it', 'PRP', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('multitude', 'NN', 'B-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('everything', 'NN', 'B-NP'),\n",
      " ('gleamed', 'VBD', 'O'),\n",
      " ('white', 'JJ', 'O'),\n",
      " ('.', '.', 'O'),\n",
      " ('The', 'DT', 'B-NP'),\n",
      " ('lightning', 'NN', 'I-NP'),\n",
      " ('split', 'VBD', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('whole', 'JJ', 'I-NP'),\n",
      " ('sky', 'NN', 'I-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('half', 'NN', 'B-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('that', 'DT', 'B-NP'),\n",
      " ('moment', 'NN', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('it', 'PRP', 'O'),\n",
      " ('was', 'VBD', 'O'),\n",
      " ('brighter', 'RBR', 'O'),\n",
      " ('than', 'IN', 'O'),\n",
      " ('daylight', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('The', 'DT', 'B-NP'),\n",
      " ('tops', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('gravestones', 'NNS', 'O'),\n",
      " ('seemed', 'VBD', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('pulse', 'VB', 'O'),\n",
      " ('like', 'IN', 'O'),\n",
      " ('strobe', 'NN', 'B-NP'),\n",
      " ('lights', 'NNS', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('night', 'NN', 'I-NP'),\n",
      " ('club', 'NN', 'B-NP'),\n",
      " ('before', 'IN', 'O'),\n",
      " ('blackness', 'NN', 'B-NP'),\n",
      " ('settled', 'VBD', 'O'),\n",
      " ('them', 'PRP', 'O'),\n",
      " ('down', 'RP', 'O'),\n",
      " ('again', 'RB', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = '''The storm had felt like a rumor all day, but now, the sky was delivering. \n",
    "For a second, like a knife catching a glint of light and refracting it in multitude, everything gleamed white. \n",
    "The lightning split the whole sky in half, and in that moment, it was brighter than daylight. \n",
    "The tops of the gravestones seemed to pulse like strobe lights in a night club before blackness settled them down again.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)\n",
    "#idee man sollte nun die Begriffe automatisiert auslesen können => mach tman mit tagging\n",
    "# RB =>  Adverb. Examples: very, silently,\n",
    "# RP => Particle. Example: give up\n",
    "# PRP => Personal Pronoun. Examples: I, he, she\n",
    "# VBD => Verb, Past Tense. Example: took\n",
    "# NN => Noun, Singular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The car is driven on the road. The truck is driven on the highway. The bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The car is driven on the road.'\n",
    "d2 = 'The truck is driven on the highway.'\n",
    "d3 = 'The bicycle is driven on the bicycle path.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization (Aufteilung in einzelne Elemente aus den Sätzen)\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road. the truck is driven on the highway. the bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function (Grund: man möchte möglichst alles einheitlich haben im Text)\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road the truck is driven on the highway the bicycle is driven on the bicycle path'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'very', 'shan', 'an', 'what', 'myself', 'itself', 'my', 'herself', 'had', \"couldn't\", 'yourselves', \"you'll\", \"weren't\", \"you'd\", 'here', 'was', 'again', 'to', 'mustn', 'the', \"should've\", 'then', 'same', 'down', 'before', 'they', 'than', 'we', 'have', 'those', 'am', 'out', 'ain', 'with', 'above', 'hasn', 'ma', 'a', 'his', \"doesn't\", \"mustn't\", \"isn't\", \"won't\", 'yourself', 'why', 'theirs', 'now', 'other', 're', 'while', 'ourselves', 'on', 'just', 'o', 'our', 'at', 'up', 'as', 'most', 'will', \"shan't\", 'your', 'them', \"hasn't\", 'nor', 'so', 'such', 'her', 'aren', 't', 'should', 'only', \"wouldn't\", 'after', 'and', 'if', \"she's\", 'doing', 'themselves', 'their', 'been', 'weren', 'because', 'all', 'but', 'y', 'isn', 'both', 'be', 'himself', 'this', 'below', \"hadn't\", 'do', \"you're\", 'you', 'through', 'that', 'during', 'hadn', \"needn't\", 'shouldn', 'does', 'once', 'who', \"wasn't\", \"it's\", 'whom', 'from', 'no', 'these', \"aren't\", 'it', 'its', 'not', 'haven', 'yours', 'ours', 'don', 'where', 'll', 'having', 'were', \"you've\", 'some', 'he', 'any', 'over', 'between', 'd', 'didn', 'hers', \"that'll\", 'of', 'him', 'won', \"mightn't\", \"haven't\", \"shouldn't\", 'needn', 'can', 've', 'are', 'doesn', 'me', 'she', 'i', 'has', 'off', 'few', 's', 'couldn', 'in', 'for', 'into', 'is', 'wasn', \"didn't\", 'each', 'mightn', 'how', 'by', 'further', 'against', 'wouldn', 'own', 'more', 'when', 'did', 'until', 'or', \"don't\", 'which', 'too', 'm', 'there', 'being', 'under', 'about'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\janin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\janin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path'] \n",
      "\n",
      "After lemmatization:\n",
      "['car', 'drive', 'road', 'truck', 'drive', 'highway', 'bicycle', 'drive', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "# durch die Lemmatisierung wurde er Satz auf folgende Hauptwörter eingeschränkt => die Anführungsstriche sollten nur am Anfang/Ende sein (damit es erkennt, dass es ein Satz ist)\n",
    "corpus = ['car drive road', \n",
    "          'truck drive highway', \n",
    "          'bicycle drive bicycle path']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with different ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle  car  drive  highway  path  road  truck\n",
      "0        0    1      1        0     0     1      0\n",
      "1        0    0      1        1     0     0      1\n",
      "2        2    0      1        0     1     0      0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1) - Unigramm = N-Gramm\n",
    "vectorizer = CountVectorizer(min_df=0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)\n",
    "\n",
    "#bycicle kommt zweimal vor | drive kommt in allen 3 Dkomenten vor etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle drive  bicycle path  car drive  drive bicycle  drive highway  \\\n",
      "0              0             0          1              0              0   \n",
      "1              0             0          0              0              1   \n",
      "2              1             1          0              1              0   \n",
      "\n",
      "   drive road  truck drive  \n",
      "0           1            0  \n",
      "1           0            1  \n",
      "2           0            0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2) mit 2- hat man zwei folgewörter (B-Gramm)\n",
    "vectorizer = CountVectorizer(min_df=0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 7 \n",
      "\n",
      "The words in the corpus: \n",
      " {'highway', 'bicycle', 'road', 'car', 'truck', 'path', 'drive'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   highway  bicycle    road     car   truck  path   drive\n",
      "0   0.0000      0.0  0.3333  0.3333  0.0000  0.00  0.3333\n",
      "1   0.3333      0.0  0.0000  0.0000  0.3333  0.00  0.3333\n",
      "2   0.0000      0.5  0.0000  0.0000  0.0000  0.25  0.2500\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))\n",
    " \n",
    "# TF=> wie oft wöter vorkommen (wörter die häufig vokommen, werden heruntergewichtet, die die weniger vorkommen werden höher gewichtet)\n",
    "# am höchsten gewichtet wird das wort \"bycicle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "        highway:     0.4771\n",
      "        bicycle:     0.4771\n",
      "           road:     0.4771\n",
      "            car:     0.4771\n",
      "          truck:     0.4771\n",
      "           path:     0.4771\n",
      "          drive:        0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   highway  bicycle   road    car  truck    path  drive\n",
      "0    0.000   0.0000  0.159  0.159  0.000  0.0000    0.0\n",
      "1    0.159   0.0000  0.000  0.000  0.159  0.0000    0.0\n",
      "2    0.000   0.2386  0.000  0.000  0.000  0.1193    0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\janin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = '''European authorities fined Google a record $5.1 \n",
    "          billion on Wednesday for abusing its power in the \n",
    "          mobile phone market.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)\n",
    "#idee man sollte nun die Begriffe automatisiert auslesen können => mach tman mit tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "NT\n",
      "Windows | 10\n",
      "Datetime: 2022-11-30 12:48:03\n",
      "Python Version: 3.9.7\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
